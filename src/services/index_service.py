import os
import csv
import logging
from elasticsearch import AsyncElasticsearch

from settings import ES_HOST, ES_PORT, ES_INDEX, ES_AUTH
from services.embedding_service import EmbeddingService

logging.basicConfig(level=logging.INFO)
logging.getLogger('elastic_transport.transport').setLevel(logging.WARNING)

class IndexService:
    def __init__(self):
        self.es = AsyncElasticsearch(hosts=[f'http://{ES_HOST}:{ES_PORT}'], basic_auth=(ES_AUTH['username'], ES_AUTH['password']), request_timeout=600)
        self.index_name = ES_INDEX
        self.embedding_service = EmbeddingService()
        self.data_dir = 'data/amazon_dataset_2023'

    async def drop_index(self):
        try:
            await self.es.indices.delete(index=self.index_name)
            logging.info("Index '%s' dropped.", self.index_name)
        except Exception as e: # pylint: disable=W0703
            logging.error("Error dropping index '%s': %s", self.index_name, e)

    async def create_index(self):
        try:
            await self.es.indices.create(index=self.index_name, body={
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0
                },
                "mappings": {
                    "properties": {
                        "name": {
                            "type": "text",
                            "fields": {
                                "keyword": {
                                    "type": "keyword"
                                }
                            }
                        },
                        "main_category": {"type": "text"},
                        "sub_category": {"type": "text"},
                        "image": {"type": "text"},
                        "link": {"type": "text"},
                        "ratings": {"type": "float"},
                        "no_of_ratings": {"type": "integer"},
                        "discount_price": {"type": "text"},
                        "actual_price": {"type": "text"},
                        "name_embedding": {"type": "dense_vector", "dims": self.embedding_service.model.get_sentence_embedding_dimension()}
                    }
                }
            })
            logging.info("Index '%s' created.", self.index_name)
        except Exception as e: # pylint: disable=W0703
            logging.error("Error creating index '%s': %s", self.index_name, e)

    async def index_products(self,file_path):
        """
        Indexes products in the given CSV file.

        The function reads the given CSV file and uses the bulk API to index the products in the Elasticsearch index.
        The function assumes that the CSV file has columns named 'name', 'main_category', 'sub_category', 'image', 'link',
        'ratings', 'no_of_ratings', 'discount_price', and 'actual_price'.

        :param file_path: The path to the CSV file to read from.
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as fp:
                reader = csv.DictReader(fp)
                bulk_data = []
                for row in reader:
                    bulk_data.append({"index": {"_index": self.index_name}})
                    bulk_data.append({
                        "name": row["name"],
                        "main_category": row["main_category"],
                        "sub_category": row["sub_category"],
                        "image": row["image"],
                        "link": row["link"],
                        "ratings": row["ratings"],
                        "no_of_ratings": row["no_of_ratings"],
                        "discount_price": row["discount_price"],
                        "actual_price": row["actual_price"]
                    })
                if bulk_data:
                    await self.es.bulk(body=bulk_data)
            logging.info("Products in '%s' indexed (embeddings pending).", file_path)
        except Exception as e: # pylint: disable=W0703
            logging.error("Error indexing products in '%s': %s", file_path, e)


    async def update_embeddings(self,file_path):
        """
        Updates embeddings in products in the given CSV file.

        The function reads the given CSV file, finds the corresponding documents in the Elasticsearch index,
        and updates the 'name_embedding' field with the dense vector generated by the embedding service.

        :param file_path: The path to the CSV file to read from.
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as fp:
                reader = csv.DictReader(fp)
                bulk_data = []
                for row in reader:
                    result = await self.es.search(index=self.index_name, body={"query": {"term": {"name.keyword": row["name"]}}})
                    if result['hits']['total']['value'] > 0:
                        doc_id = result['hits']['hits'][0]['_id']
                        if 'name_embedding' not in result['hits']['hits'][0]['_source'] or not result['hits']['hits'][0]['_source']['name_embedding']:
                            embedding = await self.embedding_service.get_embedding(row["name"])
                            if embedding is not None:
                                bulk_data.append({"update": {"_index": self.index_name, "_id": doc_id}})
                                bulk_data.append({"doc": {"name_embedding": embedding.tolist()}})
                            else:
                                logging.warning("Could not generate embedding for title: %s", row['name'])
                if bulk_data:
                    await self.es.bulk(body=bulk_data)
            logging.info("Embeddings in '%s' updated.", file_path)
        except Exception as e: # pylint: disable=W0703
            logging.error("Error updating embeddings in '%s': %s", file_path, e)


    async def index_all(self):
        """
        Indexes all CSV files in the configured data directory.

        This function reads all CSV files in the configured data directory, indexes the products in them,
        and updates the 'name_embedding' field with the dense vector generated by the embedding service.

        :raises Exception: If an error occurred while indexing the files.
        """
        try:
            files = [os.path.join(self.data_dir, file) for file in os.listdir(self.data_dir) if file.endswith('.csv')]
            for file in files:
                await self.index_products(file)
            logging.info("All files in '%s' fully indexed.", self.data_dir)

        except Exception as e: # pylint: disable=W0703
            logging.error("Error indexing files in '%s': %s", self.data_dir, e)

    async def udpate_embeddings_all(self):
        """
        Updates the embeddings for all CSV files in the configured data directory.

        This function iterates over all CSV files in the specified data directory and updates the 'name_embedding'
        field with the dense vector generated by the embedding service for each product.

        :raises Exception: If an error occurs while updating embeddings in the files.
        """
        try:
            files = [os.path.join(self.data_dir, file) for file in os.listdir(self.data_dir) if file.endswith('.csv')]
            for file in files:
                await self.update_embeddings(file)
            logging.info("All files in '%s' fully updated embeddings.", self.data_dir)

        except Exception as e: # pylint: disable=W0703
            logging.error("Error indexing files in '%s': %s", self.data_dir, e)


    async def reindex(self):
        """
        Reindexes the Elasticsearch index.

        This function drops the existing index, creates a new index with the current
        mappings and settings, and indexes all CSV files in the configured data directory.
        The function returns a success message upon successful reindexing or logs an error
        and returns a failure message if an exception occurs.

        :return: "Success" if reindexing is successful, "Failed" otherwise.
        """

        try:
            await self.drop_index()
            await self.create_index()
            await self.index_all()
            return "Success"
        except Exception as e: # pylint: disable=W0703
            logging.error("Error during reindexing: %s", e)
            return "Failed"
        finally:
            await self.es.close()